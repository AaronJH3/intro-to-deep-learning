{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Exercise\n",
    "\n",
    "In this exercise you'll implement a neural network to perform segmentation on the Oxford Pets dataset (or another dataset, if you prefer...). The provided notebook is an acceptable solution, but perhaps you can do better by building a model from scratch, starting from an alternative model, or importing a model designed to perform segmentation such as UNet. \n",
    "\n",
    "### Step 1: Download and prepare the data\n",
    "\n",
    "The Oxford Pets dataset can be found here: [http://www.robots.ox.ac.uk/~vgg/data/pets/](http://www.robots.ox.ac.uk/~vgg/data/pets/)\n",
    "\n",
    "Download the dataset and the ground truth data. Then, read and process the images and their matching trimap masks. You can leverage the code from the Jupyter notebook extensively to do this.\n",
    "\n",
    "During this process you will have to change the way training data is prepared from the trimaps from what is in the notebook. Specifically, you should create three output feature maps (one for each: dog, cat and background) instead of two (one for dog, one for cat). The feature maps for the \"dog\" and \"cat\" layer will be identical to the ones in the notebook, and all three will still only contain values of 0 or 1, where zero indicates \"this pixel is not a member of this feature-map's class\" and one indicates \"this pixel is a member of this feature-map's class\". The new feature-map will contain 1's for pixels that are a part of the background, and 0's for pixels that are NOT a part of the background.\n",
    "\n",
    "### Step 2: Build the model\n",
    "\n",
    "You can again leverage much of the code in the notebook, but critically to support the changes we made to our training data you will have to update the number of kernels (from 2 to 3) and activation function (from sigmoid to softmax) in the final layer. You'll also need to change the loss function (from binary cross entropy to categorial cross entropy).\n",
    "\n",
    "### Step 3: Train and evaluate the model\n",
    "\n",
    "Did your changes result in any significant improvements on the models performance on this dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports:\n",
    "from collections import namedtuple\n",
    "import csv\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Conv2DTranspose, add as keras_add\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to download and prepare the data. We're using the Oxford Pet dataset\n",
    "# linked above. You need the \"dataset\" and the \"groundtruth data\" listed under Downloads\n",
    "\n",
    "# Modify these to fit wherever you save the data.\n",
    "PATH_TO_IMAGES = './OxfordPets/images/'\n",
    "PATH_TO_CLASS_LIST = './OxfordPets/annotations/list.txt'\n",
    "PATH_TO_TRIMAPS = './OxfordPets/annotations/trimaps'\n",
    "\n",
    "# Change if you wish. \n",
    "TARGET_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should look familiar by now. We removed the bounding box specific stuff.\n",
    "# We will be using this process on BOTH the input images and the trimaps.\n",
    "def resize_image(path_to_image, target_size=None, pad_with_value=0):\n",
    "    image = Image.open(path_to_image)\n",
    "    width, height = image.size\n",
    "    \n",
    "    w_pad = 0\n",
    "    h_pad = 0\n",
    "    bonus_h_pad = 0\n",
    "    bonus_w_pad = 0\n",
    "    \n",
    "    if width > height:\n",
    "        pix_diff = (width - height)\n",
    "        h_pad = pix_diff // 2\n",
    "        bonus_h_pad = pix_diff % 2 # If the difference was odd, add one pixel on one side.\n",
    "    elif height > width:\n",
    "        pix_diff = (height - width)\n",
    "        w_pad = pix_diff // 2\n",
    "        bonus_w_pad = pix_diff % 2 # If the difference was odd, add one pixel on one side.\n",
    "    # else: image is already square. Both pads stay 0\n",
    "    \n",
    "    image = ImageOps.expand(image, (w_pad, h_pad, w_pad+bonus_w_pad, h_pad+bonus_h_pad), pad_with_value)\n",
    "    \n",
    "    if target_size is not None:\n",
    "        # Note, width and height have changed due to the padding resize.\n",
    "        # Update our notions so we get the scale factor right\n",
    "        width, height = image.size\n",
    "\n",
    "        image = image.resize(target_size)\n",
    "        \n",
    "        width_scale = target_size[0] / width\n",
    "        height_scale = target_size[1] / height\n",
    "\n",
    "    # This is a change, this function now handles data with either 1 or 3 color \n",
    "    # channels. We can detect this from the shape of the np.array so we've broken\n",
    "    # this line into 3. \n",
    "    image_as_array = np.array(image.getdata())\n",
    "    if len(image_as_array.shape) == 2:\n",
    "        # This is required to handle transparency, which some of the JPGs contain.\n",
    "        # We are essentially removing the alpha channel. \n",
    "        if image_as_array.shape[1] == 4:\n",
    "            image_as_array = image_as_array[:, 0:3]\n",
    "        image_data = image_as_array.reshape(image.size[0], image.size[1], 3)\n",
    "    else:\n",
    "        image_data = image_as_array.reshape(image.size[0], image.size[1])\n",
    "    \n",
    "    # Image data is a 3D array, 3 channels (RGB) of target_size.\n",
    "    # RBG values are from 0-255. Later in this notebook we preprocess\n",
    "    # those images with the MobileNetV2 preprocess input function.     \n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a reshaped trimap, reduce it such that it contains\n",
    "# 2 values: 0 for \"not object of interest\" and 1 for \"object of interest\"\n",
    "\n",
    "# Note that, this transform includes the \"unclassified\" border zone\n",
    "# as part of the background. If you wanted to explicitly classify\n",
    "# the border seprately from the image you'd have to make this transform\n",
    "# more involved, and not reduce the trimap to jsut values. \n",
    "\n",
    "# Similarly, if you had examples with dogs and cats in the same image\n",
    "# This reduction step would have to account for those values differently\n",
    "def reduce_trimap_values(trimap):\n",
    "    return np.where((trimap == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the relevant data from a row of the CSV return a reshaped image\n",
    "# as well the reshaped trimap\n",
    "def prepare_sample_from_name(sample_name):\n",
    "    path_to_image = os.path.join(PATH_TO_IMAGES, sample_name + '.jpg')\n",
    "    path_to_trimap = os.path.join(PATH_TO_TRIMAPS, sample_name + '.png')\n",
    "\n",
    "    image_data = resize_image(path_to_image, TARGET_SIZE)\n",
    "    \n",
    "    # Note that in OUR TRIMAPS 1 is the value for \"background\"\n",
    "    # The final value of 2 is specific to our dataset!!\n",
    "    trimap = resize_image(path_to_trimap, TARGET_SIZE, 2) \n",
    "    \n",
    "    return (image_data, trimap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot just the image, or just the trimap, by leaving them none. \n",
    "# If both are specified the trimap will be printed on top of the image\n",
    "# in a semi-transparent way. I love how easy this one is. \n",
    "def plot_with_trimap_overlay(image=None, trimap=None):\n",
    "    if image is not None:\n",
    "        plt.imshow(image)\n",
    "    \n",
    "    if trimap is not None:\n",
    "        plt.imshow(trimap, alpha=.5)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7349 samples\n"
     ]
    }
   ],
   "source": [
    "# Okay, lets get all our samples processed. After this we'll prepare the \n",
    "# data and labels for our network and perform a validation split.\n",
    "processed_data = []\n",
    "\n",
    "# Processing all this data takes some time... \n",
    "# Took my laptop roughly 8 minutes\n",
    "with open(PATH_TO_CLASS_LIST) as csv_list_file:\n",
    "    csv_reader = csv.reader(csv_list_file, delimiter=' ')\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        if row[0].startswith('#'): continue\n",
    "        \n",
    "        # Unpack for readability\n",
    "        sample_name, class_id, species, breed_id = row\n",
    "\n",
    "        # Not every image has a bounding box, some files are missing.\n",
    "        # Use a try/except block to ignore such samples\n",
    "        try:\n",
    "            image, trimap = prepare_sample_from_name(sample_name)\n",
    "        except FileNotFoundError:\n",
    "            # More images have their trimap than had their bounding box\n",
    "            # which is a small surprise.\n",
    "            print(f'No annotations for {sample_name}: skipped.')\n",
    "            continue\n",
    "        \n",
    "        # species - 1 so cat = 0 and dog = 1.\n",
    "        # Makes things a little easier to process\n",
    "        data_tuple = (image, int(species) - 1, trimap)\n",
    "        processed_data.append(data_tuple)\n",
    "        \n",
    "print(f'Processed {len(processed_data)} samples')\n",
    "\n",
    "# Make it a numpy array\n",
    "processed_data = np.array(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time around, instead of training two outputs on a single network\n",
    "# we're going to have the final output layer produce 3 activation maps\n",
    "# One activation map each for the location of cats, dogs, and background\n",
    "\n",
    "# In our dataset, for the true labels, one of these panes will always\n",
    "# be empty (all 0), because our dataset never has images with a cat and \n",
    "# a dog in the same image. \n",
    "\n",
    "# We'll test our network afterwards on a few images with both\n",
    "# dogs and cats, and see what happens... cross your fingers, but don't hold your breath!\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_validation = []\n",
    "y_validation = []\n",
    "\n",
    "validation_split = 0.2\n",
    "\n",
    "# Notice that we're applying the preprocess_input function to the images here.\n",
    "# Also note that our labels are shaped (w, h, 3), one pane each for \"cat, dog, background\"\n",
    "for image, species, trimap in processed_data:\n",
    "    processed_image = preprocess_input(image)\n",
    "    \n",
    "    # 3 panes each with binary values. Note they mutually exclusive, each pixel can be \"on\"\n",
    "    # in exactly ONE of the three following maps.\n",
    "    cat_segmentation = reduce_trimap_values(trimap) if species == 0 else np.zeros(trimap.shape)\n",
    "    dog_segmentation = reduce_trimap_values(trimap) if species == 1 else np.zeros(trimap.shape)\n",
    "    bg_segmentation = np.logical_not(reduce_trimap_values(trimap))\n",
    "    \n",
    "    if np.random.random() > validation_split:\n",
    "        x_train.append(processed_image)\n",
    "        y_train.append([cat_segmentation, dog_segmentation, bg_segmentation])\n",
    "    else:\n",
    "        x_validation.append(processed_image)\n",
    "        y_validation.append([cat_segmentation, dog_segmentation, bg_segmentation])\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_validation = np.array(x_validation)\n",
    "y_validation = np.array(y_validation)\n",
    "\n",
    "# Because of how we appeneded the cat and dog data to the labels\n",
    "# we need to change the dimensions before prediction so that the\n",
    "# color channels are where the network expects them:\n",
    "y_train = np.rollaxis(y_train, 1, 4)\n",
    "y_validation = np.rollaxis(y_validation, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Build the model¶\n",
    "You can again leverage much of the code in the notebook, but critically to support the changes we made to our training data you will have to update the number of kernels (from 2 to 3) and activation function (from sigmoid to softmax) in the final layer. You'll also need to change the loss function (from binary cross entropy to categorial cross entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't have a TON of data, but we could actually do some data augmentation\n",
    "# Because the label data is in the same format as the input. We'd have to \n",
    "# transform the labels which is not built into Keras, but might be a good exercise\n",
    "# for later... \n",
    "\n",
    "# Lets still use transfer learning\n",
    "# Like before, we grab a pretrained model with include_top=False\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3))\n",
    "\n",
    "# We're going to lop off the last few layers, which have most \n",
    "# likely learned the highest level features specific to imagenet \n",
    "chopped_mobilenet = Model(inputs=[base_model.input], outputs=[base_model.layers[90].output])\n",
    "\n",
    "# Now, we have to use \"Convolution Transpose\" sometimes called \"Deconvolution\" to \n",
    "# re-increase the resolution, since MobileNetV2 significantly reduces the input size\n",
    "# Drawing on inspiration from U-Net, we're also going to add some symmetric skip connections\n",
    "# which should make us feel *very* fancy. \n",
    "segmentation_output = Conv2DTranspose(32, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')(chopped_mobilenet.output)\n",
    "\n",
    "segmentation_output = Conv2DTranspose(24, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')(\n",
    "    keras_add([segmentation_output, base_model.layers[45].output])\n",
    ")\n",
    "\n",
    "segmentation_output = Conv2DTranspose(24, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')(\n",
    "    keras_add([segmentation_output, base_model.layers[27].output])\n",
    ")\n",
    "\n",
    "# Note: we use 3 frames, one for cat one for dog one for background, and we \n",
    "# and likely improve our result, especially with respect to generalization and finding\n",
    "# BOTH cats and dogs in the same image. Plus, it would allow us to detect the absense\n",
    "# of either cats or dogs more readily. \n",
    "segmentation_output = Conv2DTranspose(3, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='softmax')(segmentation_output)\n",
    "\n",
    "model = Model(inputs=[chopped_mobilenet.input], outputs=[segmentation_output])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
