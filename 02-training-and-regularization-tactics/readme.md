# Training and Regularization Tactics

## Part 1: Activation Functions

### Reading Material:

* [Vanishing Gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)
* [Many ReLUs](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)
* [Pros and cons of Activation functions](https://yashuseth.blog/2018/02/11/which-activation-function-to-use-in-neural-networks/)
* [Combining different activations, paper](https://arxiv.org/abs/1801.09403v1)

### Helpful documentation

* [Keras Activations](https://keras.io/activations/)
* [Keras Advanced Activations](https://keras.io/layers/advanced-activations/)
