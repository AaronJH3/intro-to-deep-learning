# Class 2: Training and Regularization Tactics

Despite what youâ€™ve heard, neural networks are not a panacea to every AI problem. They certainly provide advantages over other AI and ML tactics, such as reducing the need for feature engineering and capturing complex relationships in high dimensional data. They also have weaknesses such as huge data and computational resource requirements, lack of interpretability, and risk of overfitting.

In this class we're going to dive deeper into the most important component parts of neural networks, and explore the impact of choices we can make regarding each component.

## Part 1: Activation Functions

### Pre-Reading Material:

* [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
* [Vanishing Gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)

### Helpful documentation

* [Keras Activations](https://keras.io/activations/)
* [Keras Advanced Activations](https://keras.io/layers/advanced-activations/)
* [ML Cheatsheet: Activation Functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)

### Resources For Further Exploration

* [Pros and cons of Activation functions](https://yashuseth.blog/2018/02/11/which-activation-function-to-use-in-neural-networks/)
* [Many ReLUs](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)
* [Combining different activations, paper](https://arxiv.org/abs/1801.09403v1)
* [Benefits of ELUs, paper](https://arxiv.org/abs/1511.07289v1)

## Part 2: Loss Functions

### Pre-Reading Material

* [Common Loss Functions](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23)

### Helpful Documentation

* [Keras Built In Loss Functions](https://keras.io/losses/)
* [ML Cheatsheet: Loss functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)

### Resources for Further Exploration

* [5 Regression Loss Functions All Machine Learners Should Know](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)
* [How to Choose a Loss Function](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
* [Picking Loss Functions](https://rohanvarma.me/Loss-Functions/)
* [Building a Complex Custom Loss Function in Keras](https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618)


## Part 3: Optimizers

### Pre-Reading Material:

* [Gradient Descent: Machine Learning is Applied Calculus](https://medium.com/tebs-lab/gradient-descent-604f6d6c116d)
* [Introduction to Optimizers](https://blog.algorithmia.com/introduction-to-optimizers/)

### Helpful Documentation:

* [Keras Optimizers](https://keras.io/optimizers/)
* [ML Cheat Sheet, Optimizers](https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html)

### Resources For Further Exploration

* [Optimizing Gradient Descent](http://ruder.io/optimizing-gradient-descent/)
* [Different Optimizers in Depth](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)

## Part 4: Regularization

### Pre-Reading Material

* [A Gentle Introduction to Dropout for Regularizing Deep Neural Networks](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)
* [Early Stopping in Keras](https://chrisalbon.com/deep_learning/keras/neural_network_early_stopping/)

### Helpful Documentation

* [Keras Docs, Dropout](https://keras.io/layers/core/#dropout)
* [Keras Docs, Early Stopping](https://keras.io/callbacks/#earlystopping)

### Resources For Further Exploration

* [Deep Learning Book, Chapter 7: Regularization for Deep Learning](https://www.deeplearningbook.org/contents/regularization.html)
* [Early Stopping and Its Faults](http://alexadam.ca/ml/2018/08/03/early-stopping.html)
